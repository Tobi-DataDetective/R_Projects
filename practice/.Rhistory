#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
cleanset<- tm_map(cleanset,PlainTextDocument)
cleanset<- tm_map(cleanset,stemDocument)
cleanset<-tm_map(cleanset,stripWhitespace)
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
tdm
tdm<- as.matrix(tdm)
tdm[1:1000,1:10]
#########Bar plot########
freq<-rowSums(tdm)
freq<-subset(freq,freq>=25)
freq
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
View(jumia)
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
cleanset<- tm_map(cleanset,PlainTextDocument)
cleanset<- tm_map(cleanset,stemDocument)
cleanset
cleanset<-tm_map(cleanset,stripWhitespace)
tdm
tdm<- as.matrix(tdm)
tdm[1:1000,1:10]
#########Bar plot########
freq<-rowSums(tdm)
freq<-subset(freq,freq>=25)
freq
barplot(freq,
las = 2,
col = rainbow(40),ylab = 'tweet frequency')
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w), freq = w,
max.words = 150,random.order = F,
min.freq = 5, colors = brewer.pal(8,'Dark2'),
scale = c(5,0.3),
rot.per = 0.3)
library(wordcloud2)
w<-data.frame(names(w),w)
colnames(w)<-c('word','freq')
wordcloud2(w,
size = 0.2,
shape = 'circle')
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
inspect(cleanset[1:5])
inspect(cleanset[1:5])
leanset<- tm_map(cleanset, gsub,
pattern ='deliveryeri',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='deliv',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivered',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivering',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='deliveryeri',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='deliv',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivered',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivering',
replace = 'delivery')
cleanset
cleanset<- tm_map(cleanset,PlainTextDocument)
cleanset<- tm_map(cleanset,stemDocument)
cleanset
cleanset<-tm_map(cleanset,stripWhitespace)
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
tdm
tdm<- as.matrix(tdm)
tdm[1:1000,1:10]
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
View(jumia)
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
inspect(cleanset[1:5])
cleanset<- tm_map(cleanset,PlainTextDocument)
cleanset
cleanset<-tm_map(cleanset,stripWhitespace)
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
cleanset<- tm_map(cleanset,PlainTextDocument)
cleanset<- tm_map(cleanset,stemDocument)
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*',' ',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
cleanset<-tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
cleanset<- tm_map(cleanset,stemDocument)
inspect(cleanset[1:5])
cleanset<- tm_map(cleanset,PlainTextDocument)
inspect(cleanset[1:5])
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
tdm
tdm<- as.matrix(tdm)
tdm[1:1000,1:10]
tweets<- iconv(jumia$text)
sent<- get_nrc_sentiment(tweets)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
tweets<- iconv(jumia$text)
sent<- get_nrc_sentiment(tweets)
sent[300:400,]
tweets[46]# trust
tweets[174]# negativity
sentscore<-barplot(colSums(sent),
las = 2,
col = rainbow(20),
ylab = 'Count',
main = 'JUMIA SENTIMENT SCORES')
sentscore<-barplot(colSums(sent),
las = 2,
col = rainbow(20),
ylab = 'Count',
main = 'JUMIA SENTIMENT SCORES')
sentscore<-barplot(colSums(sent),
las = 2,
col = rainbow(20),
ylab = 'Count',
main = 'JUMIA SENTIMENT SCORES')
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
corpus<- Corpus(VectorSource(corpus))
corpus[1:5]
inspect(corpus[1:5])
#remove retweets
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
inspect(corpus[1:5])
corpus[1:5]
inspect(corpus[1:5])
jumia<-read.csv("C:/Users/Folayan Tobi/Documents/videos/sentiment project/jumia/jumia.csv")
str(jumia)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#library(sentiment)
library(RCurl)
library(tm)
library(tm)
library(twitteR)
##########building corpus########
corpus<- iconv(jumia$text)
#remove retweets
corpus[1:5]
corpus<- gsub("(RT|via)((?:\\b\\w*@\\w+)+)"," ",corpus)
#remove people
corpus<- gsub("@\\w+"," ",corpus)
corpus<- gsub("\n.*"," ",corpus) #removes every \n pattern
corpus[1:5]
######creating a corpus###############
corpus<- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
################cleaning text#####################
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,removeNumbers)
cleanset<- tm_map(corpus, removeWords,stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset<- tm_map(cleanset,content_transformer(removeURL))
cleanset<- tm_map(cleanset,removeWords,c('jumia','rt','leggend'))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
tdm
tdm<- as.matrix(tdm)
tdm[1:10,1:20]
#########Bar plot########
w<-rowSums(tdm)
#########Bar plot########
freqq<-rowSums(tdm)
#########Bar plot########
freq<-rowSums(tdm)
freq<-subset(freq,freq>=25)
barplot(freq,
las = 2,
col = rainbow(40),ylab = 'tweet frequency')
barplot(freq,
las = 2,
col = rainbow(40),ylab = 'tweet frequency')
cleanset<- tm_map(cleanset, gsub,
pattern ='deliveryeri',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='deliv',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivered',
replace = 'delivery')
cleanset<- tm_map(cleanset, gsub,
pattern ='delivering',
replace = 'delivery')
cleanset<-tm_map(cleanset,stripWhitespace)
###########Term document matrix##########
tdm<- TermDocumentMatrix(cleanset)
tdm
tdm<- as.matrix(tdm)
tdm[1:10,1:20]
#########Bar plot########
freq<-rowSums(tdm)
freq<-subset(freq,freq>=25)
freq
barplot(freq,
las = 2,
col = rainbow(40),ylab = 'tweet frequency')
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w), freq = w,
max.words = 150,random.order = F,
min.freq = 5, colors = brewer.pal(8,'Dark2'),
scale = c(5,0.3),
rot.per = 0.3)
library(wordcloud2)
w<-data.frame(names(w),w)
colnames(w)<-c('word','freq')
wordcloud2(w,
size = 0.2,
shape = 'circle')
##########wordcloud2##########
library(wordcloud2)
w <- data.frame(names(w), w)
w
colnames(w) <- c('word','freq')
head(w)
wordcloud2(w,
size = 0.5,
shape = 'pyramid',
rotateRatio = 0.5,
minSize = 1)
wordcloud2(w,
size = 0.5,
shape = 'circle',
rotateRatio = 0.5,
minSize = 1)
